# Preprocessing
clip_length:       1                         # Number of frames within a clip 
clip_offset:       0                         # Frame offset between beginning of video and clip (1st clip only) 
clip_stride:       0                         # Frame offset between successive frames
crop_shape:        [368,368]                 # (Height, Width) of frame  
crop_type:         CropClip                  # Type of cropping operation (Random, Central and None)  
final_shape:       [368,368]                 # (Height, Width) of input to be given to CNN
num_clips:         0                        # Number clips to be generated from a video (<0: uniform sampling, 0: Divide entire video into clips, >0: Defines number of clips) 
random_offset:     0                         # Boolean switch to generate a clip length sized clip from a video 
resize_shape:      [368,368]                 # (Height, Width) to resize original data 
sample_duration:   16                        # Temporal size of video to be provided as input to the model 
sample_size:       112                       # Height of frame to be provided as input to the model
subtract_mean:     [123.675, 116.28, 103.53] # Subtract mean (R,G,B) from all frames during preprocessing
divide_std:        [58.395, 57.12, 57.375]     # Divide by standard deviation (R,G,B) when subtracting mean

# Experiment Setup 
acc_metric:        Eval_PoseTrack18_det  # Accuracy metric 
batch_size:        12                    # Numbers of videos in a mini-batch 
dataset:           Surgical_Hands        # Name of dataset 
debug:             1                     # If True, do not plot, save, or create data files 
epoch:             1                     # Total number of epochs 
exp:               'eval_surgical_hands' # Experiment name
gamma:             0.1                   # Multiplier with which to change learning rate
grad_max_norm:     0                     # Norm for gradient clipping 
json_path:         './data/pub_surgical' # Path to the json file for the given dataset
labels:            2                     # Number of total classes in the dataset
load_type:         train_val             # Environment selection, to include only training/training and validation/testing dataset
loss_type:         JointsMSELoss         # Loss function
lr:                0.001                 # Learning rate
milestones:        5                     # Epoch values to change learning rate     
model:             FlowTrack_r_gt_v5_linear #Name of model to be loaded  
momentum:          0.9                   # Momentum value in optimizer
num_workers:       4                     # Number of CPU worker used to load data
opt:               adam                  # Name of optimizer
preprocess:        default               # String argument to select preprocessing type
pretrained:        0                     # Load pretrained network 
pseudo_batch_loop: 1                     # Pseudo-batch size multiplier to mimic large minibatches 
rerun:             1                     # Number of trials to repeat an experiment
save_dir:          './results'           # Path to results directory
seed:              999                   # Seed for reproducibility 
weight_decay:      0                     # Weight decay

#Config parameters unique to hand model
gaussian_sigma:    3.0   #Hands keypoint - Gaussian keypoint sigma value (default was orginally 3.0)
hand_use_max:      1 #For ground truth gaussian heatmaps, use max values if True, else use average
hand_jitter:       0 #Use color jittering
hand_rotate:       1 #Randomly rotate cropped image
hand_rotate_amount: [-40,40] #Randomly rotate between these two degrees 
hand_translate:    0 #Randomly translate cropped image between x values
hand_translate_amount: [0.25,0.25] #Max random translate in relative [x,y] scale of image
hand_scale:        0 #Randomly zoom in or out of the image
hand_scale_amount: [1.0,2.0]

#Config parameters unique to FlowTrack model
conf_threshold:    0.42
gaussian_sigma:    3.0      # Gaussian keypoint sigma value (default is 3.0)
heatmap_size:      [96,96]  # (Width, Height)
num_joints:        21       #NOTE: This should be more dataset specific
save_feat:         false # In eval.py save output features
save_feat_dir:     './features/' # Location to save extracted features
sc:                2.2 #Scale parameter for size of bounding box, expands width and height by this amount
use_target_weight: true 

#Model.Extra
deconv_with_bias:   false
final_conv_kernel:  1
num_deconv_filters: [256,256,256]
num_deconv_kernels: [4,4,4]
num_deconv_layers:  3
num_layers:         152

#Poseval evaluation for Hands.
poseval_dir: '/z/home/natlouis/poseval_hand/py'
#Pose Matching during tracking
l2_margin: 800.0     #margin for l2 matching strategy
last_T_frames: 1     #Number of frames to look back for matching
match_strategy: 'sc' #Options: sc (spatial-consistency), gcn, l2 (avg dist between keypoints)
spa_con_thresh: 0.2  #threshold for spatial consistency 

#For the recursively trained models
min_temporal_dist: 3 
prior_threshold: 0.5      # Threshold to use prediction_{t-1} or ground truth_{t-1} as prior during training
min_gauss_peak_train: 0.0 # Minimum confidence value to use as target for gaussian peaks, during training
min_gauss_peak_eval: 0.25 # Minimum confidence value to use as target for gaussian peaks, during evaluation

#For the GCN matching strategy
cont_acc_margin: 20
in_channels: 2
out_feat: False  
#gcn_checkpoint: './results/GCN/Surgical_Hands_KP_lamss5me/checkpoints/Surgical_Hands_KP_best_model.pkl'

#If using optical flow images
#flow_data_root: '/z/home/natlouis/data/pub_surgical/flow_images' #Source of optical flow generated images
